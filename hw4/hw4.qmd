---
title: "Biostat 203B Homework 4"
date: "`r format(Sys.time(), '%d %B, %Y')`"
author: "Yufan Gong (UID: 305301666)"
format:
  html:
    theme: cosmo
    embed-resources: true
    number-sections: true
    toc: true
    toc-depth: 4
    toc-location: left
    code-fold: false
knitr:
  opts_chunk: 
    cache: false    
    echo: true
    fig.align: 'center'
    fig.width: 6
    fig.height: 4
    message: FALSE
---

Display machine information:
```{r}
sessionInfo()
```

Load database libraries and the tidyverse frontend:
```{r}
suppressPackageStartupMessages(library(tidymodels))
suppressPackageStartupMessages(library(here))
suppressPackageStartupMessages(library(rlang))
suppressPackageStartupMessages(library(ranger))
suppressPackageStartupMessages(library(xgboost))
suppressPackageStartupMessages(library(gtsummary))
suppressPackageStartupMessages(library(tidyverse))

```

## Predicting 30-day mortality

Using the ICU cohort `icu_cohort.rds` you built in Homework 3, develop at least three analytic approaches for predicting the 30-day mortality of patients admitted to ICU using demographic information (gender, age, marital status, ethnicity), first lab measurements during ICU stay, and first vital measurements during ICU stay. For example, you can use (1) logistic regression with elastic net (lasso + ridge) penalty (e.g., glmnet or keras package), (2) random forest, (3) boosting, and (4) support vector machines, or (5) MLP neural network (keras package)

1. Partition data into 50% training set and 50% test set. Stratify partitioning according the 30-day mortality status.


First, we need to copy `icu_cohort.rds` from `hw3` to `hw4`.
```{r}
# Copy icu_cohort.rds from hw3 to hw4

# Set the source and destination directories
hw3_path <- here("hw3", "mimiciv_shiny")
hw4_path <- here("hw4")

# Set the file name and path
file_name <- "icu_cohort.rds"
file_path <- file.path(hw3_path, file_name)

# check if the file already exists in the destination directory
if (!file.exists(str_c(hw4_path, "/", file_name))) {
  # copy the file to the destination directory
  file.copy(from = file_path, 
            to = hw4_path)
  print("File copied successfully!")
} else {
  print("File already exists in the destination directory.")
}
```


Then, we need to make a subset of the original dataset with variables of interest and remove the outliers.
```{r}

#create functions

## quote all elements in a list
quote_all <- function(...){
  args <- ensyms(...)
  paste(map(args, as_string), sep = "")
}

## remove extreme values based on their iqrs
extreme_remove_iqr <- function(x) {
  Q = quantile(x, c(0.25, 0.75), na.rm = TRUE)
  iqr = IQR(x, na.rm = TRUE)
  x = if_else(x < Q[1] - 1.5 * iqr | x > Q[2] + 1.5 * iqr, NA, x)
}


#Load data
icu_cohort <- read_rds("icu_cohort.rds")

#only keep variables that could contribute to thirty day mortality
vars_to_drop <- quote_all(time, `_id`, dod, los, location, flag, year, 
                          unit, language, anchor, length, type, insurance)

covars <- names(icu_cohort) %>% 
  discard(
         str_detect(
           ., 
           str_c(vars_to_drop, collapse = "|")
           )
       ) 

#get the subset which only contains variables of interest
icu_cohort_new <- icu_cohort %>% 
  select(all_of(covars)) %>% 
  mutate_if(is.numeric, extreme_remove_iqr) # remove outliers

icu_cohort_new %>% 
  tbl_summary(by = thirty_day_mort, missing = "no") %>% 
  add_n() %>% 
      add_overall() %>% 
      modify_header(label = "**Characteristics**") %>%
      modify_spanning_header(
        starts_with("stat_") ~ "**Thirty day mortality**"
      ) %>%
  modify_caption("Table 1. Characteristics of MIMIC IV") %>% 
  bold_labels() %>% 
  as_flex_table()

```


After data cleaning, we can split the data into `training` set and `testing` set for analysis.
```{r}
#Split data into test and training set
data_split <- initial_split(
  icu_cohort_new, 
  # stratify by 30-day mortality status
  strata = "thirty_day_mort", 
  prop = 0.5
  )

data_split

list(training, testing) %>% 
  map(~ data_split %>% 
        .x) %>% 
  set_names("icu_cohort_training", "icu_cohort_testing") %>% 
  list2env(.GlobalEnv) %>% 
  invisible()

dim(icu_cohort_training)
dim(icu_cohort_testing)
```


2. Train and tune the models using the training set.

I used logistic regression, random forest, and boosting to train the models. I intended to impute the predictors since I removed some outliers. The existence of extreme values could just be due to entry error which is common in such a big database. 
```{r}
rf_recipe <- 
  recipe(
    thirty_day_mort ~ ., 
    data = icu_cohort_training
  ) %>%
  # mean imputation for numeric_var
  step_impute_mean(all_numeric_predictors()) %>%
  # mode imputation for categorical_var
  step_impute_mode(all_nominal_predictors()) %>%
  # # create traditional dummy variables (not necessary for random forest in R)
  # step_dummy(all_nominal_predictors()) %>%
  # zero-variance filter
  step_zv(all_numeric_predictors()) 
  # # center and scale numeric data (not necessary for random forest)
  # step_normalize(all_numeric_predictors()) %>%
rf_recipe

logit_recipe <- 
  rf_recipe %>% 
  step_dummy(all_nominal_predictors()) %>%
  # # center and scale numeric data (not necessary for random forest)
  step_normalize(all_numeric_predictors()) 
logit_recipe

gb_recipe <- 
  rf_recipe %>%
  # create traditional dummy variables (necessary for xgboost)
  step_dummy(all_nominal_predictors()) 
gb_recipe
```


```{r}
# Models
logit_mod <- 
  logistic_reg(
    penalty = tune(), 
    mixture = tune()
  ) %>% 
  set_engine("glmnet", standardize = FALSE)
logit_mod

rf_mod <- 
  rand_forest(
    mode = "classification",
    # Number of predictors randomly sampled in each split
    mtry = tune(),
    # Number of trees in ensemble
    trees = tune()
  ) %>% 
  set_engine("ranger")
rf_mod

gb_mod <- 
  boost_tree(
    mode = "classification",
    trees = 1000, 
    tree_depth = tune(),
    learn_rate = tune()
  ) %>% 
  set_engine("xgboost")
gb_mod
```


```{r}
# Workflows
list(
  list(logit_recipe, rf_recipe, gb_recipe), 
  list(logit_mod, rf_mod, gb_mod)
  ) %>% 
  pmap(
    ~ workflow() %>% 
      add_recipe(.x) %>% 
      add_model(.y)
  ) %>% 
  set_names("logit_wf", "rf_wf", "gb_wf") %>% 
  list2env(.GlobalEnv) %>% 
  invisible()

logit_wf
rf_wf
gb_wf

```


```{r}
# Tuning grid
param_grid_logit <- grid_regular(
  penalty(range = c(-6, 3)), 
  mixture(),
  levels = c(100, 5)
  )
param_grid_logit

param_grid_rf <- grid_regular(
  trees(range = c(100L, 300L)), 
  mtry(range = c(1L, 5L)),
  levels = c(3, 5)
  )
param_grid_rf

param_grid_boost <- grid_regular(
  tree_depth(range = c(1L, 3L)),
  learn_rate(range = c(-5, 2), trans = log10_trans()),
  levels = c(3, 10)
  )
param_grid_boost
```

```{r}
set.seed(203)

folds <- vfold_cv(icu_cohort_training, v = 5)
folds
```

```{r}
list(
  list(logit_wf, rf_wf, gb_wf), 
  list(param_grid_logit, param_grid_rf, param_grid_boost)
  ) %>% 
  pmap(
    ~ .x %>% 
      tune_grid(
        resamples = folds, 
        grid = .y, 
        metrics = metric_set(roc_auc, accuracy)
        )
    ) %>% 
  set_names("logit_fit", "rf_fit", "gb_fit") %>% 
  list2env(.GlobalEnv) %>% 
  invisible()

logit_fit
rf_fit
gb_fit

```

```{r}

logit_fit %>%
  collect_metrics() %>%
  print(width = Inf) %>%
  filter(.metric == "roc_auc") %>%
  ggplot(mapping = aes(x = penalty, y = mean, color = mixture)) +
  geom_point() +
  labs(x = "Penalty", y = "CV AUC") +
  scale_x_log10()

rf_fit %>%
  collect_metrics() %>%
  print(width = Inf) %>%
  filter(.metric == "roc_auc") %>%
  ggplot(mapping = aes(x = trees, y = mean, color = mtry)) +
  geom_point() + 
  # geom_line() + 
  labs(x = "Num. of Trees", y = "CV AUC")

gb_fit %>%
  collect_metrics() %>%
  print(width = Inf) %>%
  filter(.metric == "roc_auc") %>%
  ggplot(mapping = aes(x = learn_rate, y = mean, color = tree_depth)) +
  geom_point() +
  labs(x = "Learning Rate", y = "CV AUC") +
  scale_x_log10()
```

```{r}

list(logit_fit, rf_fit, gb_fit) %>% 
  map(
    ~ .x %>% 
      show_best("roc_auc")
    )

```

```{r}

list(logit_fit, rf_fit, gb_fit) %>% 
  map(
    ~ .x %>% 
      select_best("roc_auc")
    ) %>% 
  set_names("best_logit", "best_rf", "best_gb") %>% 
  list2env(.GlobalEnv) %>% 
  invisible()

best_logit
best_rf
best_gb

```


3. Compare model classification performance on the test set. Report both the area under ROC curve and accuracy for each model.

```{r}
# Final workflows
list(
  list(logit_wf, rf_wf, gb_wf), 
  list(best_logit, best_rf, best_gb)
  ) %>% 
  pmap(
    ~ .x %>% 
      finalize_workflow(.y)
  ) %>% 
  set_names("final_wf_logit", "final_wf_rf", "final_wf_gb") %>% 
  list2env(.GlobalEnv) %>% 
  invisible()

final_wf_logit
final_wf_rf
final_wf_gb
```

```{r}
#Fit the whole training set, then predict the test cases
list(final_wf_logit, final_wf_rf, final_wf_gb) %>% 
  map(
    ~ .x %>% 
      last_fit(data_split)
    ) %>% 
  set_names("final_fit_logit", "final_fit_rf", "final_fit_gb") %>% 
  list2env(.GlobalEnv) %>% 
  invisible()

final_fit_logit
final_fit_rf
final_fit_gb
```

```{r}
# Test metrics
list(final_fit_logit, final_fit_rf, final_fit_gb) %>% 
  map(collect_metrics)

```
